import cloudscraper
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
from deep_translator import GoogleTranslator

class HavenNestCrawler:
    def __init__(self):
        self.all_listings = []
        self.scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})
        self.translator = GoogleTranslator(source='auto', target='zh-CN')

    def force_clean(self, text):
        """ç»ˆææ¸…ç†ï¼šå½»åº•ç§»é™¤å¯¼è‡´ VS Code æŠ¥é”™çš„æ‰€æœ‰éæ³•å­—ç¬¦"""
        if not text: return ""
        # ç§»é™¤ LS (\u2028), PS (\u2029) ä»¥åŠæ§åˆ¶å­—ç¬¦
        clean_text = re.sub(r'[\u2028\u2029\u0000-\u001f\u007f-\u009f]', '', text)
        return " ".join(clean_text.split())

    def ai_translate(self, text):
        if not text: return ""
        try:
            return self.translator.translate(self.force_clean(text)[:300])
        except:
            return self.force_clean(text)

    def crawl_craigslist(self, limit=10):
        print("ğŸ” æ­£åœ¨æŠ“å– Craigslist å¹¶è¿›è¡Œ AI æ±‰åŒ–...")
        url = "https://vancouver.craigslist.org/search/apa"
        try:
            res = self.scraper.get(url, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            items = soup.find_all('li', class_='cl-static-search-result')[:limit]
            
            for item in items:
                link = item.find('a')['href']
                title = self.force_clean(item.find('div', class_='title').text)
                
                # æ·±åº¦æŠ“å›¾
                d_res = self.scraper.get(link, timeout=30)
                d_soup = BeautifulSoup(d_res.text, 'html.parser')
                img_el = d_soup.find('img')
                img_url = img_el['src'] if img_el else "https://images.unsplash.com/photo-1560518883-ce09059eeffa?w=800"

                self.all_listings.append({
                    "source": "Craigslist",
                    "native_lang": "en",
                    "title": title,
                    "title_cn": self.ai_translate(title),
                    "price": item.find('div', class_='price').text if item.find('div', class_='price') else "é¢è®®",
                    "url": link,
                    "location": self.force_clean(item.find('div', class_='location').text) if item.find('div', class_='location') else "Vancouver",
                    "image": img_url
                })
                print(f"âœ… [Craigslist] å·²ç¿»è¯‘: {title[:15]}...")
                time.sleep(1)
        except Exception as e:
            print(f"âŒ Craigslist å¼‚å¸¸: {e}")

    def crawl_zumper(self, limit=10):
        print("ğŸ” æ­£åœ¨è½¬å‘æŠ“å– Zumper é«˜è´¨é‡æˆ¿æº...")
        url = "https://www.zumper.com/apartments-for-rent/vancouver-bc"
        try:
            res = self.scraper.get(url, timeout=20)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # Zumper çš„æˆ¿æºå¡ç‰‡é€šå¸¸åœ¨ç‰¹å®šçš„ Feed ç±»ä¸­
            items = soup.select('[data-testid="listing-card"]')[:limit]
            
            for item in items:
                title_el = item.select_one('[class*="Title"]')
                link_el = item.select_one('a[href*="/apartments-for-rent/"]')
                price_el = item.select_one('[class*="Price"]')
                
                if title_el and link_el:
                    title = self.force_clean(title_el.text)
                    self.all_listings.append({
                        "source": "Zumper",
                        "native_lang": "en",
                        "title": title,
                        "title_cn": self.ai_translate(title),
                        "price": price_el.text if price_el else "é¢è®®",
                        "url": "https://www.zumper.com" + link_el['href'],
                        "location": "Vancouver",
                        "image": "https://images.unsplash.com/photo-1512917774080-9991f1c4c750?w=800" # Zumper éœ€ç‰¹æ®Šå¤„ç†å›¾ç‰‡æµï¼Œå…ˆç”¨é«˜æ¸…å ä½
                    })
                    print(f"âœ… [Zumper] å·²å¤„ç†: {title[:15]}...")
            
            print(f"âœ¨ Zumper è·å–æˆåŠŸï¼Œå¢åŠ  {len(self.all_listings)} æ¡ç²¾é€‰æˆ¿æºã€‚")
        except Exception as e:
            print(f"âŒ Zumper æŠ“å–å¤±è´¥: {e}")

    def save(self):
        with open('listings.json', 'w', encoding='utf-8') as f:
            json_str = json.dumps(self.all_listings, ensure_ascii=False, indent=4)
            # ç‰©ç†çº§å‰”é™¤å¼‚å¸¸è¡Œç»ˆæ­¢ç¬¦
            cleaned_json = re.sub(r'[\u2028\u2029]', '', json_str)
            f.write(cleaned_json)
        print(f"ğŸ“Š ä»»åŠ¡ç»“æŸï¼šJSON æ–‡ä»¶å·²å½»åº•æ¸…æ´—å¹¶å­˜å…¥ {len(self.all_listings)} æ¡æ•°æ®ã€‚")

if __name__ == "__main__":
    crawler = HavenNestCrawler()
    crawler.crawl_craigslist(limit=10) 
    crawler.crawl_zumper(limit=10)
    crawler.save()