import cloudscraper
from bs4 import BeautifulSoup
import json
import os
import re
import time
from datetime import datetime, timedelta
from deep_translator import GoogleTranslator

class HavenNestCrawler:
    def __init__(self):
        self.filename = 'listings.json'
        self.all_listings = self.load_existing_data()
        self.seen_urls = {item['url'] for item in self.all_listings}
        # ğŸš€ å¢å¼ºç‰ˆä¼ªè£…ï¼Œé™ä½è¢«å°é”æ¦‚ç‡ [cite: 2026-02-28]
        self.scraper = cloudscraper.create_scraper(
            browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True},
            delay=10
        )
        self.translator = GoogleTranslator(source='auto', target='zh-CN')

    def load_existing_data(self):
        if os.path.exists(self.filename):
            try:
                with open(self.filename, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data if isinstance(data, list) else []
            except: return []
        return []

    def clean_old_data(self, days=45):
        cutoff_date = datetime.now() - timedelta(days=days)
        initial_count = len(self.all_listings)
        self.all_listings = [
            item for item in self.all_listings 
            if datetime.strptime(item.get('date', datetime.now().strftime("%Y-%m-%d")), '%Y-%m-%d') > cutoff_date
        ]
        print(f"ğŸ§¹ è‡ªåŠ¨æ¸…ç†ï¼šå·²ç§»é™¤ {initial_count - len(self.all_listings)} æ¡é™ˆæ—§æˆ¿æºã€‚")

    def ai_translate(self, text):
        if not text or len(text) < 5: return text # é˜²æ­¢ç¿»è¯‘é”™è¯¯ç‰‡æ®µ
        try:
            return self.translator.translate(text[:200])
        except: return text

    def crawl_craigslist(self, limit=20):
        print(f"ğŸ” æ­£åœ¨æŠ“å– Craigslist (åŒ…å«å®æ‹å›¾è¯†åˆ«)...")
        url = "https://vancouver.craigslist.org/search/apa"
        try:
            # ğŸš€ å¢åŠ è¶…æ—¶æ—¶é—´è‡³ 30s
            res = self.scraper.get(url, timeout=30)
            if "blocked" in res.text.lower():
                print("âŒ è¢« Craigslist æš‚æ—¶æ‹¦æˆªï¼Œè·³è¿‡æœ¬æ¬¡æŠ“å–ã€‚")
                return

            soup = BeautifulSoup(res.text, 'html.parser')
            items = soup.find_all('li', class_='cl-static-search-result')
            count = 0
            for item in items:
                link = item.find('a')['href']
                if link in self.seen_urls: continue
                
                title = item.find('div', class_='title').text.strip()
                if "ç¬¬" in title and "ç« " in title: continue # è¿‡æ»¤æŠ“å–é”™è¯¯çš„è„æ•°æ®

                # ğŸš€ æ”¹è¿›å›¾ç‰‡æŠ“å–é€»è¾‘
                img_ids = item.get('data-ids', '').split(',')
                img_url = ""
                if img_ids and img_ids[0]:
                    clean_id = img_ids[0].replace('1:', '')
                    img_url = f"https://images.craigslist.org/{clean_id}_300x225.jpg"

                self.all_listings.insert(0, {
                    "source": "Craigslist",
                    "title": title,
                    "title_cn": self.ai_translate(title),
                    "price": item.find('div', class_='price').text if item.find('div', class_='price') else "N/A",
                    "url": link,
                    "location": "Vancouver",
                    "image": img_url,
                    "date": datetime.now().strftime("%Y-%m-%d")
                })
                self.seen_urls.add(link)
                count += 1
                if count >= limit: break
        except Exception as e: print(f"âŒ Craigslist å¼‚å¸¸: {e}")

    def crawl_zumper(self, limit=20):
        print(f"ğŸ” æ­£åœ¨åŒæ­¥ Zumper (å¢å¼ºå›¾ç‰‡å…¼å®¹æ€§)...")
        url = "https://www.zumper.com/apartments-for-rent/vancouver-bc"
        try:
            res = self.scraper.get(url, timeout=30)
            soup = BeautifulSoup(res.text, 'html.parser')
            items = soup.select('[data-testid="listing-card"]')
            count = 0
            for item in items:
                link_el = item.select_one('a[href*="/apartments-for-rent/"]')
                if not link_el: continue
                full_url = "https://www.zumper.com" + link_el['href']
                if full_url in self.seen_urls: continue

                title_el = item.select_one('[class*="Title"]')
                img_el = item.find('img')
                # ğŸš€ æŠ“å– Zumper çœŸå®å›¾ï¼Œè‹¥æ²¡æœ‰åˆ™ç•™ç©ºè§¦å‘å‰ç«¯å…œåº•
                img_url = img_el['src'] if (img_el and 'src' in img_el.attrs) else ""

                self.all_listings.insert(0, {
                    "source": "Zumper",
                    "title": title_el.text.strip() if title_el else "Vancouver Suite",
                    "title_cn": self.ai_translate(title_el.text) if title_el else "ç²¾é€‰å…¬å¯“",
                    "price": item.select_one('[class*="Price"]').text if item.select_one('[class*="Price"]') else "N/A",
                    "url": full_url,
                    "location": "Vancouver",
                    "image": img_url,
                    "date": datetime.now().strftime("%Y-%m-%d")
                })
                self.seen_urls.add(full_url)
                count += 1
                if count >= limit: break
        except Exception as e: print(f"âŒ Zumper å¼‚å¸¸: {e}")

    def save(self):
        self.clean_old_data() 
        # ğŸš€ æœ€ç»ˆæ•°æ®å»é‡ [cite: 2026-02-28]
        unique_data = []
        seen = set()
        for x in self.all_listings:
            if x['url'] not in seen:
                unique_data.append(x)
                seen.add(x['url'])
        
        with open(self.filename, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, ensure_ascii=False, indent=4)
        print(f"ğŸ“Š æ•°æ®åº“åŒæ­¥å®Œæˆï¼šå½“å‰å…±ç§¯æ”’ {len(unique_data)} æ¡å¸¦å›¾æˆ¿æºã€‚")

if __name__ == "__main__":
    c = HavenNestCrawler()
    c.crawl_craigslist(25)
    time.sleep(5) # ğŸš€ åœé¡¿5ç§’é˜²æ­¢è¯·æ±‚è¿‡å¿« [cite: 2026-02-28]
    c.crawl_zumper(25)
    c.save()