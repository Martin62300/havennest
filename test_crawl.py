import cloudscraper
from bs4 import BeautifulSoup
import json
import time
import re
from deep_translator import GoogleTranslator

class HavenNestCrawler:
    def __init__(self):
        self.all_listings = []
        self.seen_urls = set() # ğŸ‘ˆ å¼ºåŒ–å»é‡é€»è¾‘
        self.scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})
        self.translator = GoogleTranslator(source='auto', target='zh-CN')

    def clean_text(self, text):
        if not text: return ""
        # å½»åº•è§£å†³å¼‚å¸¸è¡Œç»ˆæ­¢ç¬¦æŠ¥é”™
        text = re.sub(r'[\u2028\u2029\u0000-\u001f\u007f-\u009f]', '', text)
        return " ".join(text.split())

    def ai_translate(self, text):
        if not text: return ""
        try:
            # è¿™é‡Œçš„æ±‰åŒ–ä¼šæ›´æ³¨é‡æ„è¯‘
            return self.translator.translate(self.clean_text(text)[:300])
        except:
            return text

    def crawl_source(self, name, url, item_selector, limit=12):
        print(f"ğŸ” æ­£åœ¨åŒæ­¥ {name} æˆ¿æº...")
        try:
            res = self.scraper.get(url, timeout=20)
            soup = BeautifulSoup(res.text, 'html.parser')
            # æ ¹æ®æ¥æºé€‰æ‹©ä¸åŒçš„è§£æé€»è¾‘
            if name == "Craigslist":
                items = soup.find_all('li', class_='cl-static-search-result')
            else: # Zumper
                items = soup.select('[data-testid="listing-card"]')
            
            count = 0
            for item in items:
                link_el = item.find('a', href=True)
                if not link_el: continue
                full_url = link_el['href'] if link_el['href'].startswith('http') else f"https://www.{name.lower()}.com{link_el['href']}"
                
                if full_url in self.seen_urls: continue # ğŸ‘ˆ ç‰©ç†å»é‡
                
                title_text = item.find('div', class_='title').text if name == "Craigslist" else item.select_one('[class*="Title"]').text
                title = self.clean_text(title_text)

                self.all_listings.append({
                    "source": name,
                    "title": title,
                    "title_cn": self.ai_translate(title), # ğŸ‘ˆ æ±‰åŒ–æ ‡é¢˜
                    "price": self.clean_text(item.find('div', class_='price').text if name == "Craigslist" else item.select_one('[class*="Price"]').text),
                    "url": full_url,
                    "location": self.clean_text(item.find('div', class_='location').text if name == "Craigslist" else "Vancouver"),
                    "image": item.find('img')['src'] if item.find('img') else "https://images.unsplash.com/photo-1560518883-ce09059eeffa?w=800"
                })
                self.seen_urls.add(full_url)
                count += 1
                if count >= limit: break
                time.sleep(1)
        except Exception as e:
            print(f"âŒ {name} å¼‚å¸¸: {e}")

    def save(self):
        with open('listings.json', 'w', encoding='utf-8') as f:
            json.dump(self.all_listings, f, ensure_ascii=False, indent=4)
        print(f"ğŸ“Š ä»»åŠ¡ç»“æŸï¼šHavennest å·²å­˜å…¥ {len(self.all_listings)} æ¡æ•°æ®ã€‚")

if __name__ == "__main__":
    crawler = HavenNestCrawler()
    crawler.crawl_source("Craigslist", "https://vancouver.craigslist.org/search/apa", None)
    crawler.crawl_source("Zumper", "https://www.zumper.com/apartments-for-rent/vancouver-bc", None)
    crawler.save()